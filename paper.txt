Neuron-Level Knowledge Attribution in Large Language Models [EMNLP 2024] [https://arxiv.org/abs/2312.12141] [Zeping Yu, et al.] [neuron]

A Primer on the Inner Workings of Transformer-based Language Models [/] [https://arxiv.org/abs/2405.00208] [Javier Ferrando, et al.] [REVIEW]

A Mathematical Framework for Transformer Circuits [/] [https://transformer-circuits.pub/2021/framework/index.html] [Nelson Elhage, et al.] [Anthropic]

Towards Monosemanticity: Decomposing Language Models With Dictionary Learning [/] [https://transformer-circuits.pub/2023/monosemantic-features/index.html] [Trenton Bricken, et al.] [Anthropic]

An Intuitive Explanation of Sparse Autoencoders for LLM Interpretability [/] [https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html] [Adam Karvonen] [SAE]

俯视LLM的灵魂：一文搞懂稀疏自动编码器 [https://www.51cto.com/aigc/1251.html] [庞德公] [SAE, blog]

Interpretable Machine Learning - A Guide for Making Black Box Models Explainable [/] [https://christophm.github.io/interpretable-ml-book/] [Christoph Molnar] [book]

